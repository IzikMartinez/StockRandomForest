{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-06T01:44:06.903925Z",
     "start_time": "2024-04-06T01:44:06.893320Z"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "\"\"\"\n",
    "Closing Price of stock stockDF on day T\n",
    "Represents Sj(t)\n",
    "\"\"\"\n",
    "def closing_price(stockDF, T):\n",
    "    return stockDF.loc[str(T), 'Close']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Daily Rate of Return.\n",
    "Represents Yj(t)\n",
    "\"\"\"\n",
    "def daily_rate_of_return(stockDF, T):\n",
    "    today = closing_price(stockDF, T)\n",
    "    yesterday = closing_price(stockDF, T - 1)\n",
    "    return (today - yesterday) / yesterday\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Vj\n",
    "Creates a list of data corresponding to two weeks of trading days before the day T\n",
    "\"\"\"\n",
    "def vj(stockDF, T):\n",
    "    rates_of_return = []\n",
    "    for i in range(0, 15):\n",
    "        rates_of_return.append(daily_rate_of_return(stockDF, T - i))\n",
    "    # Invert the elements in rates_of_return\n",
    "    rates_of_return = rates_of_return[:: -1]\n",
    "    return rates_of_return\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "True Class\n",
    "returns 0, 1, 2 based on the value of the input\n",
    "Corresponds to STABLE, UP and DOWN\n",
    "\"\"\"\n",
    "def true_class(targ):\n",
    "    # if UP\n",
    "    if targ >= 0.006:\n",
    "        return 1\n",
    "    # if DOWN\n",
    "    elif targ <= -0.006:\n",
    "        return 2\n",
    "    # If STABLE\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "### TARG is defined as tomorrows Yj(T)\n",
    "def targ(stockDF, T):\n",
    "    return daily_rate_of_return(stockDF, T+1)\n",
    "\n",
    "\n",
    "def make_zt(X):\n",
    "    return X.map(true_class)\n",
    "\n",
    "def make_xt(stocks, T):\n",
    "    \"\"\"\n",
    "    Xt Represents an array containing all our line vector stock data \"Vj(t)\"\n",
    "    \"\"\"\n",
    "    xt_list = []\n",
    "    ticker_list = []\n",
    "    for stock, ticker in stocks:\n",
    "        xt_list.append(vj(stock, T))\n",
    "        ticker_list.append(ticker)\n",
    "    return xt_list, ticker_list\n",
    "\n",
    "def create_xt(stock, T):\n",
    "    \"\"\" \n",
    "    Xt Represents an array containing all our line vector stock data \"Vj(t)\" \n",
    "    \"\"\"\n",
    "    xt_list = []\n",
    "    for stock, ticker in stocks:\n",
    "        xt_list.extend(vj(stock, T))\n",
    "    return xt_list\n",
    "\n",
    "\n",
    "def confusion_matrix_percentage(conf_matrix):\n",
    "    return (conf_matrix / conf_matrix.sum()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "################### Slide 2 #########################\n",
    "tickers = [\"AAPL\", \"MSFT\", \"TSLA\", \"META\", \"GOOGL\", \"AMZN\", \"NVDA\", \"AMD\", \"DIS\", \"NFLX\",\n",
    "           \"JPM\", \"KO\", \"BAC\", \"C\", \"WFC\", \"GS\", \"AXP\", \"MCD\", \"DJI\", \"SPY\"]\n",
    "\n",
    "stocks = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    stocks.append((yf.Ticker(ticker).history(start='2016-01-01', end='2022-12-31'), ticker))\n",
    "\n",
    "for stock in stocks:\n",
    "    new_index = [str(i + 1) for i in range(len(stock[0]))]\n",
    "    stock[0].index = new_index\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T23:54:30.709089Z",
     "start_time": "2024-04-05T23:54:20.406704Z"
    }
   },
   "id": "da2ddcd56aa73da",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "################### Slide 4 #########################\n",
    "Y = []\n",
    "for stock, ticker in stocks:\n",
    "    for day in range(2, len(stock) - 1):\n",
    "        Y.append((ticker, day, daily_rate_of_return(stock, day)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T00:10:22.329387Z",
     "start_time": "2024-04-06T00:10:21.908048Z"
    }
   },
   "id": "1d2627cb66a8f463",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6299/2961004328.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = cm.get_cmap('rainbow', 3)  # using 'rainbow' colormap here, you can choose any\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\n# Now plotting\\nfor chunk in chunks_of_tickers:\\n    for i, ticker in enumerate(chunk):\\n        ticker_data = [x for x in Y if x[0] == ticker]  # filter out data for this ticker only\\n        days = [x[1] for x in ticker_data]  # x-axis data\\n        rates_of_return = [x[2] for x in ticker_data]  # y-axis data\\n        plt.plot(days, rates_of_return, color=colors(i), label=ticker)\\n    plt.xlabel('Day')\\n    plt.ylabel('Rate of Return')\\n    plt.title('Rate of Return Over Days for Each Ticker')\\n    plt.legend()  # add a legend\\n    plt.show()\\n\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### Slide 4 cont #########################\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colors = cm.get_cmap('rainbow', 3)  # using 'rainbow' colormap here, you can choose any\n",
    "unique_tickers = list(set([x[0] for x in Y]))  # gets unique tickers\n",
    "\n",
    "\n",
    "# Chunking function\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "# Dividing tickers into chunks of 5\n",
    "chunks_of_tickers = list(chunks(unique_tickers, 3))\n",
    "\"\"\"\n",
    "# Now plotting\n",
    "for chunk in chunks_of_tickers:\n",
    "    for i, ticker in enumerate(chunk):\n",
    "        ticker_data = [x for x in Y if x[0] == ticker]  # filter out data for this ticker only\n",
    "        days = [x[1] for x in ticker_data]  # x-axis data\n",
    "        rates_of_return = [x[2] for x in ticker_data]  # y-axis data\n",
    "        plt.plot(days, rates_of_return, color=colors(i), label=ticker)\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Rate of Return')\n",
    "    plt.title('Rate of Return Over Days for Each Ticker')\n",
    "    plt.legend()  # add a legend\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T00:10:23.567482Z",
     "start_time": "2024-04-06T00:10:23.461291Z"
    }
   },
   "id": "ecd1fd607f9ab2a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5        6    \\\n",
      "0 -0.001067  0.016446 -0.010401 -0.004059 -0.005577  0.000755  0.00679   \n",
      "\n",
      "        7         8         9    ...       290       291       292      293  \\\n",
      "0 -0.009741 -0.023457  0.001993  ...  0.000826  0.012434 -0.009355  0.00029   \n",
      "\n",
      "        294       295       296       297       298       299  \n",
      "0 -0.008714  0.009866 -0.009334  0.000293 -0.003465  0.006317  \n",
      "\n",
      "[1 rows x 300 columns]\n",
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "################### Slide 5 #########################\n",
    "Xt, tickers = make_xt(stocks, 97)\n",
    "mod_xt = create_xt(stocks, 97)\n",
    "# extract vj values and convert to dataframe\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_data = pd.DataFrame(mod_xt).T\n",
    "\"\"\"\n",
    "# reshape it into 1D and convert into a DataFrame\n",
    "X_data = pd.DataFrame(Xt, index=tickers).T\n",
    "\"\"\"\n",
    "\n",
    "print(X_data)\n",
    "print(X_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T00:10:26.435162Z",
     "start_time": "2024-04-06T00:10:26.419612Z"
    }
   },
   "id": "bd6c79a74661b202",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  290  291  292  293  \\\n",
      "0    0    1    2    0    0    0    1    2    2    0  ...    0    1    2    0   \n",
      "\n",
      "   294  295  296  297  298  299  \n",
      "0    2    1    2    0    0    1  \n",
      "\n",
      "[1 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "################### Slide 6 #########################\n",
    "Z_data = make_zt(X_data)\n",
    "print(Z_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T00:10:28.924201Z",
     "start_time": "2024-04-06T00:10:28.903612Z"
    }
   },
   "id": "4ca6ec247e62c600",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "################### Slide 7 #########################\n",
    "# Slide 7 simply describes the goal of the next few slides\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:41:27.943430Z",
     "start_time": "2024-04-04T21:41:27.939196Z"
    }
   },
   "id": "670efdc3104b6023",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "112\n",
      "90\n",
      "98\n",
      "Initial Stable Ratio: 0.37333333333333335\n",
      "Initial Up Ratio: 0.3\n",
      "Initial Down Ratio: 0.32666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nDue to the data set already containing a fairly balanced set of classes, there is no need to clone the data in X_data\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### Slide 8 #########################\n",
    "\n",
    "CL0 = Z_data.apply(lambda x: (x == 0))\n",
    "CL1 = Z_data.apply(lambda x: (x == 1))\n",
    "CL2 = Z_data.apply(lambda x: (x == 2))\n",
    "\n",
    "s0 = CL0.sum().sum()\n",
    "s1 = CL1.sum().sum()\n",
    "s2 = CL2.sum().sum()\n",
    "\n",
    "N = s0 + s1 + s2\n",
    "\n",
    "print(N)\n",
    "print(s0)\n",
    "print(s1)\n",
    "print(s2)\n",
    "\n",
    "R0 = s0/N\n",
    "R1 = s1/N\n",
    "R2 = s2/N\n",
    "print(f\"Initial Stable Ratio: {R0}\\nInitial Up Ratio: {R1}\\nInitial Down Ratio: {R2}\")\n",
    "\n",
    "\"\"\"\n",
    "Due to the data set already containing a fairly balanced set of classes, there is no need to clone the data in X_data\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T00:10:35.989274Z",
     "start_time": "2024-04-06T00:10:35.895223Z"
    }
   },
   "id": "227ab78f9fd115a2",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "################### Slide 9 part 1 #########################\n",
    "# Take all but the last day of trading in X_train\n",
    "X_data_mod = X_data.iloc[:, :-1]\n",
    "# Select stock 20 as our target for Random Forest Classifier\n",
    "Z_data_mod = Z_data.iloc[:, 1:]\n",
    "M = X_data.shape[1]\n",
    "print(M)\n",
    "\n",
    "# X_train and Y_train are now dataframes whose elements are offset by one day, producing a training set of a stock's daily value and tomorrow's price action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:32.502317Z",
     "start_time": "2024-04-06T20:52:32.499638Z"
    }
   },
   "id": "bba30d30e47a96e0",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 1)\n",
      "(45, 1)\n",
      "(253, 1)\n",
      "(45, 1)\n"
     ]
    }
   ],
   "source": [
    "############# Feedback correction ##########\n",
    "\"\"\"\n",
    "    As per feedback, the data set has been split into training and test data, with training data being 85% of the original size and test data being 15% of the original size\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming that X_data is your pandas DataFrame\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data_mod.T, Z_data_mod.T, test_size=0.15)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:35.318198Z",
     "start_time": "2024-04-06T20:52:35.313018Z"
    }
   },
   "id": "5f6f637f0c677c93",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 0.14035534858703613 seconds.\n",
      "OOB : 37.5%\n"
     ]
    }
   ],
   "source": [
    "################### Slide 9 part 2 #########################\n",
    "# 2M/3 proves that M must be a number less than 100, as max_samples must be a number smaller than n_estimators\n",
    "# I.E., M cannot be a number in the range 300, for a random forest with 100 trees\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "# Create our random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, max_features=18, oob_score=True)\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, np.ravel(Y_train))\n",
    "end_time = time.time()\n",
    "\n",
    "oob_score = clf.oob_score_\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Training took {elapsed_time} seconds.\")\n",
    "\n",
    "print(f\"OOB : {round(oob_score*100,1)}%\")\n",
    "z=clf.predict(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:39.394272Z",
     "start_time": "2024-04-06T20:52:39.246037Z"
    }
   },
   "id": "cc8974f91787986e",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "################### Slide 10 part 1 #########################\n",
    "# Create a reuseable Train Random Forest function to be used for the remaining slides\n",
    "\n",
    "import time\n",
    "\n",
    "def TrainRandomForest(num_tree, num_select_features, X_train, Y_train):\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    # Create our random forest classifier\n",
    "    clf = RandomForestClassifier(n_estimators=num_tree, random_state=42, oob_score=True,\n",
    "                                 bootstrap=True, max_samples=0.87,\n",
    "                                 max_features=num_select_features)\n",
    "    # Calculate computation start time\n",
    "    start_time = time.time()\n",
    "    # Train the model\n",
    "    clf.fit(X_train, np.ravel(Y_train))\n",
    "    # Calculate computation end time\n",
    "    end_time = time.time()\n",
    "    # Total time is end - start\n",
    "    elapsed_time = end_time - start_time\n",
    "    # Get out of bag score\n",
    "    oob_score = clf.oob_score_\n",
    "    return clf, oob_score, elapsed_time, clf.feature_importances_\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:44.180620Z",
     "start_time": "2024-04-06T20:52:44.177770Z"
    }
   },
   "id": "a09cabaaf5983239",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time was 0.13576769828796387 seconds\n",
      "OOB : 37.5%\n"
     ]
    }
   ],
   "source": [
    "################### Slide 10 part 2 #########################\n",
    "# Compute one iteration of the Random Forest\n",
    "\n",
    "clf, oob_score, elapsed_time, features = TrainRandomForest(100, 18, X_train, Y_train)\n",
    "print(f\"Training time was {elapsed_time} seconds\")\n",
    "\n",
    "print(f\"OOB : {round(oob_score*100,1)}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:59.443450Z",
     "start_time": "2024-04-06T20:52:59.301195Z"
    }
   },
   "id": "f86fa9712e9ae1ad",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Confusion Matrix (%):\n",
      " [[38.74% 0.00% 0.40%]\n",
      " [0.00% 29.25% 0.00%]\n",
      " [0.00% 0.00% 31.62%]]\n",
      "Test Confusion Matrix (%):\n",
      " [[8.89% 6.67% 11.11%]\n",
      " [11.11% 8.89% 13.33%]\n",
      " [17.78% 8.89% 13.33%]]\n"
     ]
    }
   ],
   "source": [
    "################### Slide 10 part 3 #########################\n",
    "# Compute the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\"\"\"Get trainCONF\"\"\"\n",
    "# Get the predicted values\n",
    "Y_train_pred = clf.predict(X_train)\n",
    "#Create the training confusion matrix\n",
    "trainCONF = confusion_matrix(np.ravel(Y_train), Y_train_pred)\n",
    "trainCONF = confusion_matrix_percentage(trainCONF)\n",
    "\n",
    "\"\"\"Get trainCONF\"\"\"\n",
    "# Get the predicted test values\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "#Create the test confusion matrix\n",
    "testCONF = confusion_matrix(np.ravel(Y_test), Y_test_pred)\n",
    "testCONF = confusion_matrix_percentage(testCONF)\n",
    "\n",
    "# Format print to print the matrix elements as percentages\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{:0.2f}%\".format(x)})\n",
    "\n",
    "print('Train Confusion Matrix (%):\\n', trainCONF)\n",
    "print('Test Confusion Matrix (%):\\n', testCONF)\n",
    "\n",
    "# Undo formatting of numbers as percent\n",
    "np.set_printoptions(formatter={'float': None})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T23:36:38.662561Z",
     "start_time": "2024-04-06T23:36:38.596361Z"
    }
   },
   "id": "1eab26d4ca7e6778",
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Box scores\n",
      " SF       18      36      72      140     200\n",
      "Tree                                        \n",
      "100   37.55%  37.55%  37.55%  37.55%  37.55%\n",
      "200   36.76%  36.76%  36.76%  36.76%  36.76%\n",
      "300   36.76%  36.76%  36.76%  36.76%  36.76%\n",
      "400   37.15%  37.15%  37.15%  37.15%  37.15%\n",
      "500   37.15%  37.15%  37.15%  37.15%  37.15%\n",
      "Computation times (s)\n",
      " SF         18        36        72        140       200\n",
      "Tree                                                  \n",
      "100   0.119599  0.095453  0.102666  0.150097  0.124453\n",
      "200   0.191713  0.194607  0.191193  0.192690  0.191122\n",
      "300   0.282931  0.283561  0.283229  0.285970  0.282841\n",
      "400   0.381158  0.382740  0.490882  0.390307  0.377627\n",
      "500   0.471941  0.471732  0.473788  0.472434  0.472232\n"
     ]
    }
   ],
   "source": [
    "################### Slide 11 #########################\n",
    "trees = [100,200,300,400,500]\n",
    "selected_features = [18,36,72,140,200]\n",
    "OOB_accuracies = []\n",
    "computation_times = []\n",
    "data = []\n",
    "for tree in trees:\n",
    "    for SF in selected_features:\n",
    "        clf, oob_score, elapsed_time, features = TrainRandomForest(tree, SF, X_train, Y_train)\n",
    "        data.append((tree, SF, f\"{round(oob_score*100,2)}%\", elapsed_time))\n",
    "\n",
    "# print(data)\n",
    "df = pd.DataFrame(data, columns=['Tree', 'SF', 'oob_score', 'computation_time'])\n",
    "df_pivot_oob = df.pivot(index='Tree', columns='SF', values='oob_score')\n",
    "df_pivot_time = df.pivot(index='Tree', columns='SF', values='computation_time')\n",
    "\n",
    "print(f\"Out of Box scores\\n {df_pivot_oob}\")\n",
    "print(f\"Computation times (s)\\n {df_pivot_time}\")\n",
    "# Best TR* = 100; Best SF* = 36"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T20:58:21.131294Z",
     "start_time": "2024-04-06T20:58:13.578050Z"
    }
   },
   "id": "95d45851f27a311b",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB : 37.9%\n"
     ]
    }
   ],
   "source": [
    "################### Slide 12-14 #########################\n",
    "from sklearn.inspection import permutation_importance\n",
    "RF_prime = RandomForestClassifier(n_estimators=100, max_features=18, criterion='gini', n_jobs=-1, oob_score=True)\n",
    "RF_prime.fit(X_train, np.ravel(Y_train))\n",
    "\n",
    "oob_prime = RF_prime.oob_score_\n",
    "features = RF_prime.n_features_in_\n",
    "importances = RF_prime.feature_importances_\n",
    "\n",
    "# Print the feature ranking\n",
    "print(f\"OOB : {round(oob_prime*100,1)}%\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T20:58:40.690915Z",
     "start_time": "2024-04-06T20:58:40.508024Z"
    }
   },
   "id": "37b8aff8148615d1",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances (Mean Decrease Accuracy):\n",
      " [-0.013 -0.14  -0.027 -0.02  -0.04  -0.067  0.    -0.023 -0.084 -0.027\n",
      " -0.074 -0.05  -0.064 -0.02  -0.037 -0.013 -0.057 -0.033 -0.02  -0.023\n",
      " -0.03  -0.043 -0.037 -0.05  -0.04  -0.02  -0.003 -0.05  -0.067 -0.047\n",
      " -0.033 -0.06  -0.054 -0.064 -0.043 -0.057 -0.05  -0.003 -0.037 -0.01\n",
      " -0.017 -0.047 -0.037 -0.043 -0.017 -0.054 -0.033 -0.03  -0.033 -0.007\n",
      " -0.06  -0.08  -0.017 -0.064 -0.047 -0.01  -0.077 -0.02  -0.057 -0.064\n",
      " -0.05  -0.043 -0.02  -0.05  -0.087 -0.043 -0.077 -0.06  -0.05  -0.033\n",
      " -0.09  -0.087 -0.003 -0.074 -0.017 -0.037 -0.084 -0.06  -0.023  0.007\n",
      " -0.037 -0.01  -0.11  -0.03  -0.02  -0.054 -0.094 -0.02  -0.04  -0.08\n",
      " -0.01  -0.057 -0.033 -0.04  -0.087 -0.023 -0.03  -0.01  -0.054 -0.054\n",
      "  0.003 -0.013 -0.057 -0.043 -0.06  -0.084 -0.043 -0.067 -0.084 -0.06\n",
      " -0.087 -0.017  0.007 -0.054 -0.11  -0.057 -0.06  -0.003 -0.03  -0.033\n",
      " -0.013 -0.077 -0.094 -0.043 -0.03  -0.043 -0.127 -0.02  -0.023  0.013\n",
      " -0.064 -0.06  -0.02  -0.007 -0.02  -0.027 -0.057 -0.043 -0.023 -0.017\n",
      "  0.017 -0.077 -0.064 -0.084 -0.07  -0.047 -0.06  -0.074 -0.05  -0.09\n",
      " -0.043 -0.06  -0.01  -0.05  -0.017 -0.033 -0.054 -0.074  0.027 -0.08\n",
      " -0.027 -0.067 -0.043 -0.1   -0.02  -0.077 -0.013  0.01  -0.05  -0.02\n",
      " -0.064 -0.03  -0.054 -0.05  -0.057 -0.013 -0.057 -0.07  -0.05  -0.04\n",
      " -0.064 -0.027 -0.043  0.01  -0.043 -0.08  -0.097 -0.023 -0.04  -0.02\n",
      " -0.11  -0.09  -0.054 -0.04  -0.03  -0.043 -0.037  0.01  -0.007  0.023\n",
      " -0.067 -0.08  -0.027  0.017  0.007 -0.043 -0.074 -0.077 -0.08  -0.033\n",
      " -0.06  -0.017 -0.047 -0.02  -0.08  -0.03  -0.05  -0.007  0.027 -0.027\n",
      " -0.047 -0.07  -0.043 -0.13  -0.033 -0.033 -0.06  -0.01  -0.054 -0.037\n",
      " -0.017 -0.064 -0.027 -0.047 -0.067 -0.067 -0.05  -0.084 -0.023 -0.064\n",
      " -0.087 -0.047 -0.027 -0.077 -0.054 -0.023 -0.043  0.    -0.074 -0.09\n",
      " -0.05  -0.04  -0.064 -0.064 -0.064 -0.06  -0.017 -0.067 -0.023 -0.057\n",
      " -0.007 -0.02  -0.067 -0.067 -0.043 -0.043 -0.054 -0.033 -0.054  0.007\n",
      " -0.064 -0.04   0.007 -0.054 -0.09  -0.023 -0.05  -0.04  -0.05  -0.04\n",
      " -0.057 -0.064 -0.064 -0.05   0.003  0.    -0.07  -0.057 -0.054 -0.03\n",
      " -0.057 -0.043 -0.04  -0.077 -0.067 -0.09  -0.04  -0.074 -0.097]\n"
     ]
    }
   ],
   "source": [
    "################### Slide 15 part 1 #########################\n",
    "\"\"\"\n",
    "For RF*, compute the importances of all features\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Your original model accuracy\n",
    "orig_accuracy = accuracy_score(Z_data.T, RF_prime.predict(X_data.T))\n",
    "\n",
    "# Store the feature importances\n",
    "importances = []\n",
    "\n",
    "# For each feature\n",
    "for i in range(X_data.shape[1]):\n",
    "    # Create a copy of the test data\n",
    "    X_data_copy = X_data.T.copy()\n",
    "\n",
    "    for i in range(X_data_copy.shape[1]):  # assuming 'i' refers to columns\n",
    "        # Shuffle the values of the ith feature\n",
    "        X_data_copy.iloc[:, i] = shuffle(X_data_copy.iloc[:, i])\n",
    "\n",
    "    # Get the accuracy score after making this change\n",
    "    shuff_accuracy = accuracy_score(Z_data.T, RF_prime.predict(X_data_copy))\n",
    "\n",
    "    # The importance of the feature is the decrease in accuracy\n",
    "    importances.append(orig_accuracy - shuff_accuracy)\n",
    "\n",
    "# Normalize the importances to get percentages\n",
    "importances = np.array(importances)\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "# Print the importances\n",
    "print(\"Feature importances (Mean Decrease Accuracy):\\n\", importances)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T23:35:44.640009Z",
     "start_time": "2024-04-06T23:35:39.659901Z"
    }
   },
   "id": "7928baab8493962a",
   "execution_count": 196
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 0 has a drop-in accuracy of: 0.885\n",
      "The feature 1 has a drop-in accuracy of: 0.861\n",
      "The feature 2 has a drop-in accuracy of: 0.837\n",
      "The feature 3 has a drop-in accuracy of: 0.765\n",
      "The feature 4 has a drop-in accuracy of: 0.765\n",
      "The feature 5 has a drop-in accuracy of: 0.765\n",
      "The feature 6 has a drop-in accuracy of: 0.693\n",
      "The feature 7 has a drop-in accuracy of: 0.693\n",
      "The feature 8 has a drop-in accuracy of: 0.693\n",
      "The feature 9 has a drop-in accuracy of: 0.67\n",
      "The feature 10 has a drop-in accuracy of: 0.67\n",
      "The feature 11 has a drop-in accuracy of: 0.67\n",
      "The feature 12 has a drop-in accuracy of: 0.646\n",
      "The feature 13 has a drop-in accuracy of: 0.646\n",
      "The feature 14 has a drop-in accuracy of: 0.646\n",
      "The feature 15 has a drop-in accuracy of: 0.646\n",
      "The feature 16 has a drop-in accuracy of: 0.622\n",
      "The feature 17 has a drop-in accuracy of: 0.622\n",
      "The feature 18 has a drop-in accuracy of: 0.622\n",
      "The feature 19 has a drop-in accuracy of: 0.622\n",
      "The feature 20 has a drop-in accuracy of: 0.622\n",
      "The feature 21 has a drop-in accuracy of: 0.622\n",
      "The feature 22 has a drop-in accuracy of: 0.622\n",
      "The feature 23 has a drop-in accuracy of: 0.622\n",
      "The feature 24 has a drop-in accuracy of: 0.598\n",
      "The feature 25 has a drop-in accuracy of: 0.598\n",
      "The feature 26 has a drop-in accuracy of: 0.598\n",
      "The feature 27 has a drop-in accuracy of: 0.598\n",
      "The feature 28 has a drop-in accuracy of: 0.598\n",
      "The feature 29 has a drop-in accuracy of: 0.598\n",
      "The feature 30 has a drop-in accuracy of: 0.598\n",
      "The feature 31 has a drop-in accuracy of: 0.574\n",
      "The feature 32 has a drop-in accuracy of: 0.574\n",
      "The feature 33 has a drop-in accuracy of: 0.574\n",
      "The feature 34 has a drop-in accuracy of: 0.574\n",
      "The feature 35 has a drop-in accuracy of: 0.55\n",
      "The feature 36 has a drop-in accuracy of: 0.55\n",
      "The feature 37 has a drop-in accuracy of: 0.55\n",
      "The feature 38 has a drop-in accuracy of: 0.55\n",
      "The feature 39 has a drop-in accuracy of: 0.55\n",
      "The feature 40 has a drop-in accuracy of: 0.55\n",
      "The feature 41 has a drop-in accuracy of: 0.55\n",
      "The feature 42 has a drop-in accuracy of: 0.55\n",
      "The feature 43 has a drop-in accuracy of: 0.55\n",
      "The feature 44 has a drop-in accuracy of: 0.55\n",
      "The feature 45 has a drop-in accuracy of: 0.55\n",
      "The feature 46 has a drop-in accuracy of: 0.55\n",
      "The feature 47 has a drop-in accuracy of: 0.55\n",
      "The feature 48 has a drop-in accuracy of: 0.55\n",
      "The feature 49 has a drop-in accuracy of: 0.526\n",
      "The feature 50 has a drop-in accuracy of: 0.526\n",
      "The feature 51 has a drop-in accuracy of: 0.526\n",
      "The feature 52 has a drop-in accuracy of: 0.526\n",
      "The feature 53 has a drop-in accuracy of: 0.526\n",
      "The feature 54 has a drop-in accuracy of: 0.526\n",
      "The feature 55 has a drop-in accuracy of: 0.526\n",
      "The feature 56 has a drop-in accuracy of: 0.526\n",
      "The feature 57 has a drop-in accuracy of: 0.502\n",
      "The feature 58 has a drop-in accuracy of: 0.502\n",
      "The feature 59 has a drop-in accuracy of: 0.502\n",
      "The feature 60 has a drop-in accuracy of: 0.502\n",
      "The feature 61 has a drop-in accuracy of: 0.502\n",
      "The feature 62 has a drop-in accuracy of: 0.502\n",
      "The feature 63 has a drop-in accuracy of: 0.502\n",
      "The feature 64 has a drop-in accuracy of: 0.502\n",
      "The feature 65 has a drop-in accuracy of: 0.502\n",
      "The feature 66 has a drop-in accuracy of: 0.502\n",
      "The feature 67 has a drop-in accuracy of: 0.478\n",
      "The feature 68 has a drop-in accuracy of: 0.478\n",
      "The feature 69 has a drop-in accuracy of: 0.478\n",
      "The feature 70 has a drop-in accuracy of: 0.478\n",
      "The feature 71 has a drop-in accuracy of: 0.478\n",
      "The feature 72 has a drop-in accuracy of: 0.478\n",
      "The feature 73 has a drop-in accuracy of: 0.478\n",
      "The feature 74 has a drop-in accuracy of: 0.478\n",
      "The feature 75 has a drop-in accuracy of: 0.478\n",
      "The feature 76 has a drop-in accuracy of: 0.478\n",
      "The feature 77 has a drop-in accuracy of: 0.478\n",
      "The feature 78 has a drop-in accuracy of: 0.478\n",
      "The feature 79 has a drop-in accuracy of: 0.478\n",
      "The feature 80 has a drop-in accuracy of: 0.454\n",
      "The feature 81 has a drop-in accuracy of: 0.454\n",
      "The feature 82 has a drop-in accuracy of: 0.454\n",
      "The feature 83 has a drop-in accuracy of: 0.454\n",
      "The feature 84 has a drop-in accuracy of: 0.454\n",
      "The feature 85 has a drop-in accuracy of: 0.454\n",
      "The feature 86 has a drop-in accuracy of: 0.454\n",
      "The feature 87 has a drop-in accuracy of: 0.454\n",
      "The feature 88 has a drop-in accuracy of: 0.454\n",
      "The feature 89 has a drop-in accuracy of: 0.454\n",
      "The feature 90 has a drop-in accuracy of: 0.454\n",
      "The feature 91 has a drop-in accuracy of: 0.454\n",
      "The feature 92 has a drop-in accuracy of: 0.43\n",
      "The feature 93 has a drop-in accuracy of: 0.43\n",
      "The feature 94 has a drop-in accuracy of: 0.43\n",
      "The feature 95 has a drop-in accuracy of: 0.43\n",
      "The feature 96 has a drop-in accuracy of: 0.43\n",
      "The feature 97 has a drop-in accuracy of: 0.43\n",
      "The feature 98 has a drop-in accuracy of: 0.43\n",
      "The feature 99 has a drop-in accuracy of: 0.43\n",
      "The feature 100 has a drop-in accuracy of: 0.43\n",
      "The feature 101 has a drop-in accuracy of: 0.43\n",
      "The feature 102 has a drop-in accuracy of: 0.43\n",
      "The feature 103 has a drop-in accuracy of: 0.407\n",
      "The feature 104 has a drop-in accuracy of: 0.407\n",
      "The feature 105 has a drop-in accuracy of: 0.407\n",
      "The feature 106 has a drop-in accuracy of: 0.407\n",
      "The feature 107 has a drop-in accuracy of: 0.407\n",
      "The feature 108 has a drop-in accuracy of: 0.407\n",
      "The feature 109 has a drop-in accuracy of: 0.407\n",
      "The feature 110 has a drop-in accuracy of: 0.407\n",
      "The feature 111 has a drop-in accuracy of: 0.407\n",
      "The feature 112 has a drop-in accuracy of: 0.407\n",
      "The feature 113 has a drop-in accuracy of: 0.407\n",
      "The feature 114 has a drop-in accuracy of: 0.407\n",
      "The feature 115 has a drop-in accuracy of: 0.407\n",
      "The feature 116 has a drop-in accuracy of: 0.407\n",
      "The feature 117 has a drop-in accuracy of: 0.407\n",
      "The feature 118 has a drop-in accuracy of: 0.383\n",
      "The feature 119 has a drop-in accuracy of: 0.383\n",
      "The feature 120 has a drop-in accuracy of: 0.383\n",
      "The feature 121 has a drop-in accuracy of: 0.383\n",
      "The feature 122 has a drop-in accuracy of: 0.383\n",
      "The feature 123 has a drop-in accuracy of: 0.383\n",
      "The feature 124 has a drop-in accuracy of: 0.383\n",
      "The feature 125 has a drop-in accuracy of: 0.383\n",
      "The feature 126 has a drop-in accuracy of: 0.383\n",
      "The feature 127 has a drop-in accuracy of: 0.383\n",
      "The feature 128 has a drop-in accuracy of: 0.383\n",
      "The feature 129 has a drop-in accuracy of: 0.383\n",
      "The feature 130 has a drop-in accuracy of: 0.383\n",
      "The feature 131 has a drop-in accuracy of: 0.359\n",
      "The feature 132 has a drop-in accuracy of: 0.359\n",
      "The feature 133 has a drop-in accuracy of: 0.359\n",
      "The feature 134 has a drop-in accuracy of: 0.359\n",
      "The feature 135 has a drop-in accuracy of: 0.359\n",
      "The feature 136 has a drop-in accuracy of: 0.359\n",
      "The feature 137 has a drop-in accuracy of: 0.359\n",
      "The feature 138 has a drop-in accuracy of: 0.335\n",
      "The feature 139 has a drop-in accuracy of: 0.335\n",
      "The feature 140 has a drop-in accuracy of: 0.335\n",
      "The feature 141 has a drop-in accuracy of: 0.335\n",
      "The feature 142 has a drop-in accuracy of: 0.335\n",
      "The feature 143 has a drop-in accuracy of: 0.335\n",
      "The feature 144 has a drop-in accuracy of: 0.335\n",
      "The feature 145 has a drop-in accuracy of: 0.335\n",
      "The feature 146 has a drop-in accuracy of: 0.335\n",
      "The feature 147 has a drop-in accuracy of: 0.335\n",
      "The feature 148 has a drop-in accuracy of: 0.335\n",
      "The feature 149 has a drop-in accuracy of: 0.335\n",
      "The feature 150 has a drop-in accuracy of: 0.335\n",
      "The feature 151 has a drop-in accuracy of: 0.311\n",
      "The feature 152 has a drop-in accuracy of: 0.311\n",
      "The feature 153 has a drop-in accuracy of: 0.311\n",
      "The feature 154 has a drop-in accuracy of: 0.311\n",
      "The feature 155 has a drop-in accuracy of: 0.311\n",
      "The feature 156 has a drop-in accuracy of: 0.311\n",
      "The feature 157 has a drop-in accuracy of: 0.311\n",
      "The feature 158 has a drop-in accuracy of: 0.311\n",
      "The feature 159 has a drop-in accuracy of: 0.311\n",
      "The feature 160 has a drop-in accuracy of: 0.311\n",
      "The feature 161 has a drop-in accuracy of: 0.311\n",
      "The feature 162 has a drop-in accuracy of: 0.311\n",
      "The feature 163 has a drop-in accuracy of: 0.311\n",
      "The feature 164 has a drop-in accuracy of: 0.287\n",
      "The feature 165 has a drop-in accuracy of: 0.287\n",
      "The feature 166 has a drop-in accuracy of: 0.287\n",
      "The feature 167 has a drop-in accuracy of: 0.287\n",
      "The feature 168 has a drop-in accuracy of: 0.287\n",
      "The feature 169 has a drop-in accuracy of: 0.287\n",
      "The feature 170 has a drop-in accuracy of: 0.287\n",
      "The feature 171 has a drop-in accuracy of: 0.287\n",
      "The feature 172 has a drop-in accuracy of: 0.287\n",
      "The feature 173 has a drop-in accuracy of: 0.287\n",
      "The feature 174 has a drop-in accuracy of: 0.287\n",
      "The feature 175 has a drop-in accuracy of: 0.287\n",
      "The feature 176 has a drop-in accuracy of: 0.287\n",
      "The feature 177 has a drop-in accuracy of: 0.263\n",
      "The feature 178 has a drop-in accuracy of: 0.263\n",
      "The feature 179 has a drop-in accuracy of: 0.263\n",
      "The feature 180 has a drop-in accuracy of: 0.263\n",
      "The feature 181 has a drop-in accuracy of: 0.263\n",
      "The feature 182 has a drop-in accuracy of: 0.263\n",
      "The feature 183 has a drop-in accuracy of: 0.263\n",
      "The feature 184 has a drop-in accuracy of: 0.263\n",
      "The feature 185 has a drop-in accuracy of: 0.263\n",
      "The feature 186 has a drop-in accuracy of: 0.263\n",
      "The feature 187 has a drop-in accuracy of: 0.263\n",
      "The feature 188 has a drop-in accuracy of: 0.263\n",
      "The feature 189 has a drop-in accuracy of: 0.263\n",
      "The feature 190 has a drop-in accuracy of: 0.263\n",
      "The feature 191 has a drop-in accuracy of: 0.263\n",
      "The feature 192 has a drop-in accuracy of: 0.263\n",
      "The feature 193 has a drop-in accuracy of: 0.263\n",
      "The feature 194 has a drop-in accuracy of: 0.239\n",
      "The feature 195 has a drop-in accuracy of: 0.239\n",
      "The feature 196 has a drop-in accuracy of: 0.239\n",
      "The feature 197 has a drop-in accuracy of: 0.239\n",
      "The feature 198 has a drop-in accuracy of: 0.239\n",
      "The feature 199 has a drop-in accuracy of: 0.239\n",
      "The feature 200 has a drop-in accuracy of: 0.239\n",
      "The feature 201 has a drop-in accuracy of: 0.239\n",
      "The feature 202 has a drop-in accuracy of: 0.239\n",
      "The feature 203 has a drop-in accuracy of: 0.239\n",
      "The feature 204 has a drop-in accuracy of: 0.239\n",
      "The feature 205 has a drop-in accuracy of: 0.239\n",
      "The feature 206 has a drop-in accuracy of: 0.239\n",
      "The feature 207 has a drop-in accuracy of: 0.239\n",
      "The feature 208 has a drop-in accuracy of: 0.239\n",
      "The feature 209 has a drop-in accuracy of: 0.239\n",
      "The feature 210 has a drop-in accuracy of: 0.239\n",
      "The feature 211 has a drop-in accuracy of: 0.239\n",
      "The feature 212 has a drop-in accuracy of: 0.239\n",
      "The feature 213 has a drop-in accuracy of: 0.215\n",
      "The feature 214 has a drop-in accuracy of: 0.215\n",
      "The feature 215 has a drop-in accuracy of: 0.215\n",
      "The feature 216 has a drop-in accuracy of: 0.215\n",
      "The feature 217 has a drop-in accuracy of: 0.215\n",
      "The feature 218 has a drop-in accuracy of: 0.215\n",
      "The feature 219 has a drop-in accuracy of: 0.215\n",
      "The feature 220 has a drop-in accuracy of: 0.215\n",
      "The feature 221 has a drop-in accuracy of: 0.215\n",
      "The feature 222 has a drop-in accuracy of: 0.215\n",
      "The feature 223 has a drop-in accuracy of: 0.215\n",
      "The feature 224 has a drop-in accuracy of: 0.215\n",
      "The feature 225 has a drop-in accuracy of: 0.215\n",
      "The feature 226 has a drop-in accuracy of: 0.215\n",
      "The feature 227 has a drop-in accuracy of: 0.215\n",
      "The feature 228 has a drop-in accuracy of: 0.215\n",
      "The feature 229 has a drop-in accuracy of: 0.215\n",
      "The feature 230 has a drop-in accuracy of: 0.191\n",
      "The feature 231 has a drop-in accuracy of: 0.191\n",
      "The feature 232 has a drop-in accuracy of: 0.191\n",
      "The feature 233 has a drop-in accuracy of: 0.191\n",
      "The feature 234 has a drop-in accuracy of: 0.191\n",
      "The feature 235 has a drop-in accuracy of: 0.191\n",
      "The feature 236 has a drop-in accuracy of: 0.191\n",
      "The feature 237 has a drop-in accuracy of: 0.191\n",
      "The feature 238 has a drop-in accuracy of: 0.167\n",
      "The feature 239 has a drop-in accuracy of: 0.167\n",
      "The feature 240 has a drop-in accuracy of: 0.167\n",
      "The feature 241 has a drop-in accuracy of: 0.167\n",
      "The feature 242 has a drop-in accuracy of: 0.167\n",
      "The feature 243 has a drop-in accuracy of: 0.167\n",
      "The feature 244 has a drop-in accuracy of: 0.167\n",
      "The feature 245 has a drop-in accuracy of: 0.167\n",
      "The feature 246 has a drop-in accuracy of: 0.167\n",
      "The feature 247 has a drop-in accuracy of: 0.167\n",
      "The feature 248 has a drop-in accuracy of: 0.167\n",
      "The feature 249 has a drop-in accuracy of: 0.167\n",
      "The feature 250 has a drop-in accuracy of: 0.167\n",
      "The feature 251 has a drop-in accuracy of: 0.143\n",
      "The feature 252 has a drop-in accuracy of: 0.143\n",
      "The feature 253 has a drop-in accuracy of: 0.143\n",
      "The feature 254 has a drop-in accuracy of: 0.143\n",
      "The feature 255 has a drop-in accuracy of: 0.143\n",
      "The feature 256 has a drop-in accuracy of: 0.143\n",
      "The feature 257 has a drop-in accuracy of: 0.143\n",
      "The feature 258 has a drop-in accuracy of: 0.12\n",
      "The feature 259 has a drop-in accuracy of: 0.12\n",
      "The feature 260 has a drop-in accuracy of: 0.12\n",
      "The feature 261 has a drop-in accuracy of: 0.12\n",
      "The feature 262 has a drop-in accuracy of: 0.12\n",
      "The feature 263 has a drop-in accuracy of: 0.12\n",
      "The feature 264 has a drop-in accuracy of: 0.096\n",
      "The feature 265 has a drop-in accuracy of: 0.096\n",
      "The feature 266 has a drop-in accuracy of: 0.096\n",
      "The feature 267 has a drop-in accuracy of: 0.096\n",
      "The feature 268 has a drop-in accuracy of: 0.096\n",
      "The feature 269 has a drop-in accuracy of: 0.072\n",
      "The feature 270 has a drop-in accuracy of: 0.072\n",
      "The feature 271 has a drop-in accuracy of: 0.072\n",
      "The feature 272 has a drop-in accuracy of: 0.072\n",
      "The feature 273 has a drop-in accuracy of: 0.072\n",
      "The feature 274 has a drop-in accuracy of: 0.072\n",
      "The feature 275 has a drop-in accuracy of: 0.048\n",
      "The feature 276 has a drop-in accuracy of: 0.048\n",
      "The feature 277 has a drop-in accuracy of: 0.024\n",
      "The feature 278 has a drop-in accuracy of: 0.024\n",
      "The feature 279 has a drop-in accuracy of: 0.024\n",
      "The feature 280 has a drop-in accuracy of: 0.024\n",
      "The feature 281 has a drop-in accuracy of: 0.024\n",
      "The feature 282 has a drop-in accuracy of: -0.0\n",
      "The feature 283 has a drop-in accuracy of: -0.0\n",
      "The feature 284 has a drop-in accuracy of: -0.0\n",
      "The feature 285 has a drop-in accuracy of: -0.0\n",
      "The feature 286 has a drop-in accuracy of: -0.024\n",
      "The feature 287 has a drop-in accuracy of: -0.024\n",
      "The feature 288 has a drop-in accuracy of: -0.048\n",
      "The feature 289 has a drop-in accuracy of: -0.048\n",
      "The feature 290 has a drop-in accuracy of: -0.048\n",
      "The feature 291 has a drop-in accuracy of: -0.048\n",
      "The feature 292 has a drop-in accuracy of: -0.048\n",
      "The feature 293 has a drop-in accuracy of: -0.072\n",
      "The feature 294 has a drop-in accuracy of: -0.12\n",
      "The feature 295 has a drop-in accuracy of: -0.143\n",
      "The feature 296 has a drop-in accuracy of: -0.167\n",
      "The feature 297 has a drop-in accuracy of: -0.191\n",
      "The feature 298 has a drop-in accuracy of: -0.287\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Rank the features by decreasing importance\n",
    "\"\"\"\n",
    "ranked_features = sorted(zip(X_data.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print ordered feature importances\n",
    "for feature, importance in ranked_features:\n",
    "    print(f\"The feature {feature} has a drop-in accuracy of: {round(importance,3)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T23:32:58.204443Z",
     "start_time": "2024-04-06T23:32:58.199172Z"
    }
   },
   "id": "75fd56d5433fd8b8",
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`lowess=True` requires statsmodels, an optional dependency, to be installed.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[191], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mImportance\u001B[39m\u001B[38;5;124m'\u001B[39m: importances, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRank\u001B[39m\u001B[38;5;124m'\u001B[39m: ranks})\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Seaborn regplot with lowess=True for a smoother regression line\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRank\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mImportance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlowess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Set labels for the plot\u001B[39;00m\n\u001B[1;32m     20\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFeature Importance Ranked by Random Forest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Apps/Mara_HW3/.venv/lib/python3.11/site-packages/seaborn/regression.py:775\u001B[0m, in \u001B[0;36mregplot\u001B[0;34m(data, x, y, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, seed, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, dropna, x_jitter, y_jitter, label, color, marker, scatter_kws, line_kws, ax)\u001B[0m\n\u001B[1;32m    773\u001B[0m scatter_kws[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmarker\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m marker\n\u001B[1;32m    774\u001B[0m line_kws \u001B[38;5;241m=\u001B[39m {} \u001B[38;5;28;01mif\u001B[39;00m line_kws \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m copy\u001B[38;5;241m.\u001B[39mcopy(line_kws)\n\u001B[0;32m--> 775\u001B[0m \u001B[43mplotter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscatter_kws\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline_kws\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    776\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ax\n",
      "File \u001B[0;32m~/Apps/Mara_HW3/.venv/lib/python3.11/site-packages/seaborn/regression.py:384\u001B[0m, in \u001B[0;36m_RegressionPlotter.plot\u001B[0;34m(self, ax, scatter_kws, line_kws)\u001B[0m\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscatterplot(ax, scatter_kws)\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_reg:\n\u001B[0;32m--> 384\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlineplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline_kws\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;66;03m# Label the axes\u001B[39;00m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/Apps/Mara_HW3/.venv/lib/python3.11/site-packages/seaborn/regression.py:429\u001B[0m, in \u001B[0;36m_RegressionPlotter.lineplot\u001B[0;34m(self, ax, kws)\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Draw the model.\"\"\"\u001B[39;00m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;66;03m# Fit the regression model\u001B[39;00m\n\u001B[0;32m--> 429\u001B[0m grid, yhat, err_bands \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_regression\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    430\u001B[0m edges \u001B[38;5;241m=\u001B[39m grid[\u001B[38;5;241m0\u001B[39m], grid[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    432\u001B[0m \u001B[38;5;66;03m# Get set default aesthetics\u001B[39;00m\n",
      "File \u001B[0;32m~/Apps/Mara_HW3/.venv/lib/python3.11/site-packages/seaborn/regression.py:198\u001B[0m, in \u001B[0;36m_RegressionPlotter.fit_regression\u001B[0;34m(self, ax, x_range, grid)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_regression\u001B[39m(\u001B[38;5;28mself\u001B[39m, ax\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, x_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, grid\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    197\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit the regression model.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 198\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_statsmodels\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;66;03m# Create the grid for the regression\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m grid \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Apps/Mara_HW3/.venv/lib/python3.11/site-packages/seaborn/regression.py:194\u001B[0m, in \u001B[0;36m_RegressionPlotter._check_statsmodels\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m option \u001B[38;5;129;01min\u001B[39;00m options:\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, option) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _has_statsmodels:\n\u001B[0;32m--> 194\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(err\u001B[38;5;241m.\u001B[39mformat(option))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: `lowess=True` requires statsmodels, an optional dependency, to be installed."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graph the importances of each feature as a descending curve\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Unpack feature names and importances into separate lists\n",
    "features, importances = zip(*ranked_features)\n",
    "\n",
    "# Represent index as a feature rank\n",
    "ranks = np.arange(len(features)) + 1\n",
    "\n",
    "# Create Dataframe for seaborn\n",
    "data = pd.DataFrame({'Importance': importances, 'Rank': ranks})\n",
    "\n",
    "# Seaborn regplot with lowess=True for a smoother regression line\n",
    "sns.regplot(x='Rank', y='Importance', data=data, lowess=True)\n",
    "\n",
    "# Set labels for the plot\n",
    "plt.title('Feature Importance Ranked by Random Forest')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Importance (Drop in Accuracy)')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T23:32:24.166637Z",
     "start_time": "2024-04-06T23:32:24.050280Z"
    }
   },
   "id": "758a768e7a9f29b3",
   "execution_count": 191
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 91 has a drop-in accuracy of: 0.885\n",
      "The feature 202 has a drop-in accuracy of: 0.861\n",
      "The feature 219 has a drop-in accuracy of: 0.837\n",
      "The feature 115 has a drop-in accuracy of: 0.765\n",
      "The feature 238 has a drop-in accuracy of: 0.765\n",
      "The feature 279 has a drop-in accuracy of: 0.765\n",
      "The feature 24 has a drop-in accuracy of: 0.693\n",
      "The feature 148 has a drop-in accuracy of: 0.693\n",
      "The feature 156 has a drop-in accuracy of: 0.693\n",
      "The feature 99 has a drop-in accuracy of: 0.67\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the ten most important features (k1 ... k10)\n",
    "\"\"\"\n",
    "top_10_features = ranked_features[:10]\n",
    "\n",
    "for feature, importance in top_10_features:\n",
    "    print(f\"The feature {feature} has a drop-in accuracy of: {round(importance,3)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T22:49:44.043171Z",
     "start_time": "2024-04-06T22:49:44.039647Z"
    }
   },
   "id": "ae5817865bdad0f8",
   "execution_count": 156
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most important pairs of stock 'j' and time delay 'r' are:\n",
      "('NVDA', 1)\n",
      "('C', 7)\n",
      "('WFC', 9)\n",
      "('AMD', 10)\n",
      "('GS', 13)\n",
      "('DJI', 9)\n",
      "('MSFT', 9)\n",
      "('NFLX', 13)\n",
      "('JPM', 6)\n",
      "('NVDA', 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Translate our list of features into the form Yj(t-r)\n",
    " where j is a given stock, and r is the time offset\n",
    "\"\"\"\n",
    "def map_number_to_day(number: int):\n",
    "    return number%15\n",
    "    \n",
    "def map_number_to_ticker(number: int, tickers: list):\n",
    "    if number < 0:\n",
    "        raise ValueError(\"Number must be non-negative\")\n",
    "\n",
    "    index = number // 15\n",
    "    if index >= len(tickers):\n",
    "        raise ValueError(\"Number is too large for available tickers\")\n",
    "\n",
    "    return tickers[index], map_number_to_day(number)\n",
    "\n",
    "\n",
    "print(\"The ten most important pairs of stock 'j' and time delay 'r' are:\")\n",
    "for feature, importance in top_10_features:\n",
    "    print(map_number_to_ticker(feature, tickers))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T22:40:53.941836Z",
     "start_time": "2024-04-06T22:40:53.936924Z"
    }
   },
   "id": "d86229fe897c54a8",
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 92 has a drop-in accuracy of: -0.048\n",
      "The feature 157 has a drop-in accuracy of: -0.048\n",
      "The feature 162 has a drop-in accuracy of: -0.048\n",
      "The feature 224 has a drop-in accuracy of: -0.048\n",
      "The feature 68 has a drop-in accuracy of: -0.072\n",
      "The feature 267 has a drop-in accuracy of: -0.12\n",
      "The feature 193 has a drop-in accuracy of: -0.143\n",
      "The feature 230 has a drop-in accuracy of: -0.167\n",
      "The feature 286 has a drop-in accuracy of: -0.191\n",
      "The feature 160 has a drop-in accuracy of: -0.287\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Repeat for the 10 least important features\n",
    "\"\"\"\n",
    "bottom_10_features = ranked_features[-10:]\n",
    "\n",
    "for feature, importance in bottom_10_features:\n",
    "    print(f\"The feature {feature} has a drop-in accuracy of: {round(importance,3)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T22:49:57.073591Z",
     "start_time": "2024-04-06T22:49:57.070713Z"
    }
   },
   "id": "915fbb112204310c",
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten least important pairs of stock 'j' and time delay 'r' are:\n",
      "('NVDA', 2)\n",
      "('JPM', 7)\n",
      "('JPM', 12)\n",
      "('WFC', 14)\n",
      "('GOOGL', 8)\n",
      "('MCD', 12)\n",
      "('BAC', 13)\n",
      "('GS', 5)\n",
      "('SPY', 1)\n",
      "('JPM', 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "print our 10 least important features in the form (J,r)\n",
    "\"\"\"\n",
    "print(\"The ten least important pairs of stock 'j' and time delay 'r' are:\")\n",
    "for feature, importance in bottom_10_features:\n",
    "    print(map_number_to_ticker(feature, tickers))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T22:48:25.318233Z",
     "start_time": "2024-04-06T22:48:25.314481Z"
    }
   },
   "id": "f7c21cbeb4568264",
   "execution_count": 151
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.06%\n"
     ]
    }
   ],
   "source": [
    "################### Slide 16 part 1 ########################\n",
    "\"\"\"\n",
    "Keep the 40 most important features and get their Zt value\n",
    "\"\"\"\n",
    "\n",
    "# Get the 40 most important features\n",
    "features, importances = zip(*ranked_features[:40])\n",
    "# Convert into a list so we can use its elements to get the data which corresponds to each feature from X_data\n",
    "features = list(features)\n",
    "# Get the 40 most important features and their daily returns from X_data\n",
    "X40 = X_data.iloc[:,features].T\n",
    "# Get the corresponding values Zt\n",
    "Z40 = make_zt(X40)\n",
    "\n",
    "# Split the data into Training and Test data\n",
    "X40_train, X40_test, Y40_train, Y40_test = train_test_split(X40, Z40, test_size=0.15)\n",
    "\n",
    "# Train a new random forest with our RF* trees, and 20 random features per node\n",
    "RF40 = RandomForestClassifier(n_estimators=100, max_features=20, criterion='gini', n_jobs=-1, oob_score=True)\n",
    "RF40.fit(X40_train, np.ravel(Y40_train))\n",
    "\n",
    "# Print out of box score\n",
    "print(f\"{round(RF40.oob_score_*100,2)}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T23:27:08.088540Z",
     "start_time": "2024-04-06T23:27:07.964303Z"
    }
   },
   "id": "3867f3f0037f8d79",
   "execution_count": 187
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35.29% 0.00% 0.00%]\n",
      " [0.00% 35.29% 0.00%]\n",
      " [0.00% 0.00% 29.41%]]\n",
      "[[16.67% 0.00% 0.00%]\n",
      " [0.00% 33.33% 0.00%]\n",
      " [0.00% 0.00% 50.00%]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a new Training Confusion Matrix and Test Confusion Matrix \n",
    "from our Random Forest trained on the 40 most important features\n",
    "\"\"\"\n",
    "# Format print to print the matrix elements as percentages\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{:0.2f}%\".format(x)})\n",
    "\n",
    "# Get training matrix\n",
    "Y40_train_pred = RF40.predict(X40_train)\n",
    "newTrainCONF = confusion_matrix(np.ravel(Y40_train), Y40_train_pred)\n",
    "print(confusion_matrix_percentage(newTrainCONF))\n",
    "\n",
    "# Get test matrix\n",
    "Y40_test_pred = RF40.predict(X40_test)\n",
    "newTestCONF = confusion_matrix(np.ravel(Y40_test), Y40_test_pred)\n",
    "print(confusion_matrix_percentage(newTestCONF))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T23:28:39.916453Z",
     "start_time": "2024-04-06T23:28:39.881366Z"
    }
   },
   "id": "19a77e737390fa3f",
   "execution_count": 190
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
